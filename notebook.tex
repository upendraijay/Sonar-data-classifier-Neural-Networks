
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Untitled}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

This dataset consists of sonar data. The aim of this data is do
distinguish between rocks and metal structures such as sea mines on the
seafloor. The file 111 patterns obtained by bouncing sonar signals off a
metal cylinder at various angles and under various conditions and 97
patterns obtained from rocks under similar conditions.

Each pattern is a set of 60 numbers in the range 0.0 to 1.0. The label
associated with each record contains the letter ``R'' if the object is a
rock and ``M'' if it is a mine (metal cylinder).

Data can be downloded from:
https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data

    \hypertarget{loading-the-data}{%
\paragraph{Loading the data}\label{loading-the-data}}

To load the data and format it nicely, we will use two very useful
packages called Pandas and Numpy.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Importing pandas and numpy}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{to\PYZus{}categorical}
        
        \PY{c+c1}{\PYZsh{} Reading the csv file into a pandas DataFrame. As data does not contain the headings }
        \PY{c+c1}{\PYZsh{}so we provide a extra parameter header=None else first raw will be used as heading}
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://archive.ics.uci.edu/ml/machine\PYZhy{}learning\PYZhy{}databases/undocumented/connectionist\PYZhy{}bench/sonar/sonar.all\PYZhy{}data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{prefix}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Printing out the first 5 rows of data}
        \PY{n}{df}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}1}]:}        X0      X1      X2      X3      X4      X5      X6      X7      X8  \textbackslash{}
        0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   
        1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   
        2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   
        3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   
        4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   
        
               X9  {\ldots}     X51     X52     X53     X54     X55     X56     X57  \textbackslash{}
        0  0.2111  {\ldots}  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   
        1  0.2872  {\ldots}  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   
        2  0.6194  {\ldots}  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   
        3  0.1264  {\ldots}  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   
        4  0.4459  {\ldots}  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   
        
              X58     X59  X60  
        0  0.0090  0.0032    R  
        1  0.0052  0.0044    R  
        2  0.0095  0.0078    R  
        3  0.0040  0.0117    R  
        4  0.0107  0.0094    R  
        
        [5 rows x 61 columns]
\end{Verbatim}
            
    \hypertarget{splitting-the-data-into-features-and-targets-labels}{%
\section{Splitting the data into features and targets
(labels)}\label{splitting-the-data-into-features-and-targets-labels}}

Now, as a final step before the training, we'll split the data into
features (X) and targets (y).

Also, in Keras, we need to one-hot encode the output. We'll do this with
the \texttt{to\_categorical\ function}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{}create a dataframe with all training data except the target column}
        \PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X60}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}check that the target variable has been removed}
        \PY{n}{X}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}        X0      X1      X2      X3      X4      X5      X6      X7      X8  \textbackslash{}
        0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   
        1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   
        2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   
        3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   
        4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   
        
               X9  {\ldots}     X50     X51     X52     X53     X54     X55     X56  \textbackslash{}
        0  0.2111  {\ldots}  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   
        1  0.2872  {\ldots}  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   
        2  0.6194  {\ldots}  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   
        3  0.1264  {\ldots}  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   
        4  0.4459  {\ldots}  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   
        
              X57     X58     X59  
        0  0.0084  0.0090  0.0032  
        1  0.0049  0.0052  0.0044  
        2  0.0164  0.0095  0.0078  
        3  0.0044  0.0040  0.0117  
        4  0.0048  0.0107  0.0094  
        
        [5 rows x 60 columns]
\end{Verbatim}
            
    \hypertarget{as-the-target-is-a-string-value-so-we-convert-it-into-string-using-label-encoder}{%
\subsubsection{As the target is a string value so we convert it into
string using label
encoder}\label{as-the-target-is-a-string-value-so-we-convert-it-into-string-using-label-encoder}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing}
        \PY{n}{le} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}
        \PY{n}{Y}\PY{o}{=}\PY{n}{le}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{X60}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{Y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

    \end{Verbatim}

    \hypertarget{splitting-the-data-into-train-and-test}{%
\section{Splitting the data into Train and
Test}\label{splitting-the-data-into-train-and-test}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{n}{xTrain}\PY{p}{,} \PY{n}{xTest}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{yTest} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \hypertarget{feature-scaling}{%
\subsubsection{Feature Scaling}\label{feature-scaling}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
        \PY{n}{sc} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
        \PY{n}{xTrain} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{xTrain}\PY{p}{)}
        \PY{n}{xTest} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{xTest}\PY{p}{)}
\end{Verbatim}


    \hypertarget{defining-the-baseline-model-architecture}{%
\section{Defining the Baseline model
architecture}\label{defining-the-baseline-model-architecture}}

Here's where we use Keras to build our neural network.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{60}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{30}\PY{p}{,}  \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Compile model}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/300
166/166 [==============================] - 0s 1ms/step - loss: 0.7403 - acc: 0.4759
Epoch 2/300
166/166 [==============================] - 0s 104us/step - loss: 0.6000 - acc: 0.6867
Epoch 3/300
166/166 [==============================] - 0s 73us/step - loss: 0.5270 - acc: 0.7771
Epoch 4/300
166/166 [==============================] - 0s 95us/step - loss: 0.4645 - acc: 0.8373
Epoch 5/300
166/166 [==============================] - 0s 87us/step - loss: 0.4098 - acc: 0.8675
Epoch 6/300
166/166 [==============================] - 0s 86us/step - loss: 0.3609 - acc: 0.9277
Epoch 7/300
166/166 [==============================] - 0s 94us/step - loss: 0.3147 - acc: 0.9277
Epoch 8/300
166/166 [==============================] - 0s 97us/step - loss: 0.2753 - acc: 0.9337
Epoch 9/300
166/166 [==============================] - 0s 78us/step - loss: 0.2403 - acc: 0.9337
Epoch 10/300
166/166 [==============================] - 0s 93us/step - loss: 0.2079 - acc: 0.9458
Epoch 11/300
166/166 [==============================] - 0s 96us/step - loss: 0.1834 - acc: 0.9578
Epoch 12/300
166/166 [==============================] - 0s 75us/step - loss: 0.1609 - acc: 0.9639
Epoch 13/300
166/166 [==============================] - 0s 71us/step - loss: 0.1410 - acc: 0.9699
Epoch 14/300
166/166 [==============================] - 0s 89us/step - loss: 0.1234 - acc: 0.9759
Epoch 15/300
166/166 [==============================] - 0s 96us/step - loss: 0.1085 - acc: 0.9880
Epoch 16/300
166/166 [==============================] - 0s 91us/step - loss: 0.0939 - acc: 0.9880
Epoch 17/300
166/166 [==============================] - 0s 91us/step - loss: 0.0818 - acc: 0.9880
Epoch 18/300
166/166 [==============================] - 0s 101us/step - loss: 0.0719 - acc: 0.9940
Epoch 19/300
166/166 [==============================] - 0s 104us/step - loss: 0.0619 - acc: 0.9940
Epoch 20/300
166/166 [==============================] - 0s 88us/step - loss: 0.0547 - acc: 1.0000
Epoch 21/300
166/166 [==============================] - 0s 102us/step - loss: 0.0490 - acc: 1.0000
Epoch 22/300
166/166 [==============================] - 0s 92us/step - loss: 0.0421 - acc: 1.0000
Epoch 23/300
166/166 [==============================] - 0s 74us/step - loss: 0.0375 - acc: 1.0000
Epoch 24/300
166/166 [==============================] - 0s 88us/step - loss: 0.0337 - acc: 1.0000
Epoch 25/300
166/166 [==============================] - 0s 69us/step - loss: 0.0304 - acc: 1.0000
Epoch 26/300
166/166 [==============================] - 0s 74us/step - loss: 0.0265 - acc: 1.0000
Epoch 27/300
166/166 [==============================] - 0s 98us/step - loss: 0.0237 - acc: 1.0000
Epoch 28/300
166/166 [==============================] - 0s 99us/step - loss: 0.0217 - acc: 1.0000
Epoch 29/300
166/166 [==============================] - 0s 91us/step - loss: 0.0195 - acc: 1.0000
Epoch 30/300
166/166 [==============================] - 0s 96us/step - loss: 0.0177 - acc: 1.0000
Epoch 31/300
166/166 [==============================] - 0s 88us/step - loss: 0.0161 - acc: 1.0000
Epoch 32/300
166/166 [==============================] - 0s 95us/step - loss: 0.0146 - acc: 1.0000
Epoch 33/300
166/166 [==============================] - 0s 110us/step - loss: 0.0140 - acc: 1.0000
Epoch 34/300
166/166 [==============================] - 0s 89us/step - loss: 0.0125 - acc: 1.0000
Epoch 35/300
166/166 [==============================] - 0s 103us/step - loss: 0.0113 - acc: 1.0000
Epoch 36/300
166/166 [==============================] - 0s 99us/step - loss: 0.0106 - acc: 1.0000
Epoch 37/300
166/166 [==============================] - 0s 83us/step - loss: 0.0099 - acc: 1.0000
Epoch 38/300
166/166 [==============================] - 0s 114us/step - loss: 0.0090 - acc: 1.0000
Epoch 39/300
166/166 [==============================] - 0s 91us/step - loss: 0.0085 - acc: 1.0000
Epoch 40/300
166/166 [==============================] - 0s 116us/step - loss: 0.0079 - acc: 1.0000
Epoch 41/300
166/166 [==============================] - 0s 83us/step - loss: 0.0074 - acc: 1.0000
Epoch 42/300
166/166 [==============================] - 0s 110us/step - loss: 0.0070 - acc: 1.0000
Epoch 43/300
166/166 [==============================] - 0s 88us/step - loss: 0.0065 - acc: 1.0000
Epoch 44/300
166/166 [==============================] - 0s 90us/step - loss: 0.0061 - acc: 1.0000
Epoch 45/300
166/166 [==============================] - 0s 98us/step - loss: 0.0058 - acc: 1.0000
Epoch 46/300
166/166 [==============================] - 0s 99us/step - loss: 0.0055 - acc: 1.0000
Epoch 47/300
166/166 [==============================] - 0s 106us/step - loss: 0.0052 - acc: 1.0000
Epoch 48/300
166/166 [==============================] - 0s 108us/step - loss: 0.0050 - acc: 1.0000
Epoch 49/300
166/166 [==============================] - 0s 90us/step - loss: 0.0047 - acc: 1.0000
Epoch 50/300
166/166 [==============================] - 0s 115us/step - loss: 0.0045 - acc: 1.0000
Epoch 51/300
166/166 [==============================] - 0s 89us/step - loss: 0.0043 - acc: 1.0000
Epoch 52/300
166/166 [==============================] - 0s 95us/step - loss: 0.0041 - acc: 1.0000
Epoch 53/300
166/166 [==============================] - 0s 95us/step - loss: 0.0039 - acc: 1.0000
Epoch 54/300
166/166 [==============================] - 0s 98us/step - loss: 0.0037 - acc: 1.0000
Epoch 55/300
166/166 [==============================] - 0s 79us/step - loss: 0.0035 - acc: 1.0000
Epoch 56/300
166/166 [==============================] - 0s 95us/step - loss: 0.0033 - acc: 1.0000
Epoch 57/300
166/166 [==============================] - 0s 81us/step - loss: 0.0032 - acc: 1.0000
Epoch 58/300
166/166 [==============================] - 0s 97us/step - loss: 0.0031 - acc: 1.0000
Epoch 59/300
166/166 [==============================] - 0s 119us/step - loss: 0.0029 - acc: 1.0000
Epoch 60/300
166/166 [==============================] - 0s 113us/step - loss: 0.0028 - acc: 1.0000
Epoch 61/300
166/166 [==============================] - 0s 108us/step - loss: 0.0027 - acc: 1.0000
Epoch 62/300
166/166 [==============================] - 0s 95us/step - loss: 0.0026 - acc: 1.0000
Epoch 63/300
166/166 [==============================] - 0s 105us/step - loss: 0.0025 - acc: 1.0000
Epoch 64/300
166/166 [==============================] - 0s 93us/step - loss: 0.0024 - acc: 1.0000
Epoch 65/300
166/166 [==============================] - 0s 95us/step - loss: 0.0023 - acc: 1.0000
Epoch 66/300
166/166 [==============================] - 0s 93us/step - loss: 0.0022 - acc: 1.0000
Epoch 67/300
166/166 [==============================] - 0s 77us/step - loss: 0.0021 - acc: 1.0000
Epoch 68/300
166/166 [==============================] - 0s 80us/step - loss: 0.0021 - acc: 1.0000
Epoch 69/300
166/166 [==============================] - 0s 89us/step - loss: 0.0020 - acc: 1.0000
Epoch 70/300
166/166 [==============================] - 0s 82us/step - loss: 0.0019 - acc: 1.0000
Epoch 71/300
166/166 [==============================] - 0s 88us/step - loss: 0.0019 - acc: 1.0000
Epoch 72/300
166/166 [==============================] - 0s 79us/step - loss: 0.0018 - acc: 1.0000
Epoch 73/300
166/166 [==============================] - 0s 69us/step - loss: 0.0018 - acc: 1.0000
Epoch 74/300
166/166 [==============================] - 0s 87us/step - loss: 0.0017 - acc: 1.0000
Epoch 75/300
166/166 [==============================] - 0s 92us/step - loss: 0.0016 - acc: 1.0000
Epoch 76/300
166/166 [==============================] - 0s 99us/step - loss: 0.0016 - acc: 1.0000
Epoch 77/300
166/166 [==============================] - 0s 88us/step - loss: 0.0015 - acc: 1.0000
Epoch 78/300
166/166 [==============================] - 0s 90us/step - loss: 0.0015 - acc: 1.0000
Epoch 79/300
166/166 [==============================] - 0s 83us/step - loss: 0.0014 - acc: 1.0000
Epoch 80/300
166/166 [==============================] - 0s 87us/step - loss: 0.0014 - acc: 1.0000
Epoch 81/300
166/166 [==============================] - 0s 89us/step - loss: 0.0014 - acc: 1.0000
Epoch 82/300
166/166 [==============================] - 0s 82us/step - loss: 0.0013 - acc: 1.0000
Epoch 83/300
166/166 [==============================] - 0s 71us/step - loss: 0.0013 - acc: 1.0000
Epoch 84/300
166/166 [==============================] - 0s 166us/step - loss: 0.0012 - acc: 1.0000
Epoch 85/300
166/166 [==============================] - 0s 124us/step - loss: 0.0012 - acc: 1.0000
Epoch 86/300
166/166 [==============================] - 0s 80us/step - loss: 0.0012 - acc: 1.0000
Epoch 87/300
166/166 [==============================] - 0s 87us/step - loss: 0.0012 - acc: 1.0000
Epoch 88/300
166/166 [==============================] - 0s 80us/step - loss: 0.0011 - acc: 1.0000
Epoch 89/300
166/166 [==============================] - 0s 94us/step - loss: 0.0011 - acc: 1.0000
Epoch 90/300
166/166 [==============================] - 0s 83us/step - loss: 0.0011 - acc: 1.0000
Epoch 91/300
166/166 [==============================] - 0s 78us/step - loss: 0.0010 - acc: 1.0000
Epoch 92/300
166/166 [==============================] - 0s 61us/step - loss: 0.0010 - acc: 1.0000
Epoch 93/300
166/166 [==============================] - 0s 91us/step - loss: 9.8482e-04 - acc: 1.0000
Epoch 94/300
166/166 [==============================] - 0s 86us/step - loss: 9.5916e-04 - acc: 1.0000
Epoch 95/300
166/166 [==============================] - 0s 84us/step - loss: 9.3146e-04 - acc: 1.0000
Epoch 96/300
166/166 [==============================] - 0s 86us/step - loss: 9.0809e-04 - acc: 1.0000
Epoch 97/300
166/166 [==============================] - 0s 85us/step - loss: 8.8556e-04 - acc: 1.0000
Epoch 98/300
166/166 [==============================] - 0s 82us/step - loss: 8.6696e-04 - acc: 1.0000
Epoch 99/300
166/166 [==============================] - 0s 58us/step - loss: 8.4685e-04 - acc: 1.0000
Epoch 100/300
166/166 [==============================] - 0s 79us/step - loss: 8.2648e-04 - acc: 1.0000
Epoch 101/300
166/166 [==============================] - 0s 80us/step - loss: 8.0658e-04 - acc: 1.0000
Epoch 102/300
166/166 [==============================] - 0s 70us/step - loss: 7.8608e-04 - acc: 1.0000
Epoch 103/300
166/166 [==============================] - 0s 68us/step - loss: 7.6864e-04 - acc: 1.0000
Epoch 104/300
166/166 [==============================] - 0s 70us/step - loss: 7.5218e-04 - acc: 1.0000
Epoch 105/300
166/166 [==============================] - 0s 89us/step - loss: 7.3506e-04 - acc: 1.0000
Epoch 106/300
166/166 [==============================] - 0s 88us/step - loss: 7.1842e-04 - acc: 1.0000
Epoch 107/300
166/166 [==============================] - 0s 86us/step - loss: 7.0204e-04 - acc: 1.0000
Epoch 108/300
166/166 [==============================] - 0s 66us/step - loss: 6.8981e-04 - acc: 1.0000
Epoch 109/300
166/166 [==============================] - 0s 68us/step - loss: 6.7340e-04 - acc: 1.0000
Epoch 110/300
166/166 [==============================] - 0s 86us/step - loss: 6.5745e-04 - acc: 1.0000
Epoch 111/300
166/166 [==============================] - 0s 73us/step - loss: 6.4416e-04 - acc: 1.0000
Epoch 112/300
166/166 [==============================] - 0s 98us/step - loss: 6.3076e-04 - acc: 1.0000
Epoch 113/300
166/166 [==============================] - 0s 87us/step - loss: 6.1861e-04 - acc: 1.0000
Epoch 114/300
166/166 [==============================] - 0s 70us/step - loss: 6.0446e-04 - acc: 1.0000
Epoch 115/300
166/166 [==============================] - 0s 67us/step - loss: 5.9131e-04 - acc: 1.0000
Epoch 116/300
166/166 [==============================] - 0s 74us/step - loss: 5.8024e-04 - acc: 1.0000
Epoch 117/300
166/166 [==============================] - 0s 86us/step - loss: 5.6805e-04 - acc: 1.0000
Epoch 118/300
166/166 [==============================] - 0s 64us/step - loss: 5.5684e-04 - acc: 1.0000
Epoch 119/300
166/166 [==============================] - 0s 66us/step - loss: 5.4642e-04 - acc: 1.0000
Epoch 120/300
166/166 [==============================] - 0s 88us/step - loss: 5.3622e-04 - acc: 1.0000
Epoch 121/300
166/166 [==============================] - 0s 82us/step - loss: 5.2520e-04 - acc: 1.0000
Epoch 122/300
166/166 [==============================] - 0s 83us/step - loss: 5.1451e-04 - acc: 1.0000
Epoch 123/300
166/166 [==============================] - 0s 85us/step - loss: 5.0325e-04 - acc: 1.0000
Epoch 124/300
166/166 [==============================] - 0s 55us/step - loss: 4.9426e-04 - acc: 1.0000
Epoch 125/300
166/166 [==============================] - 0s 81us/step - loss: 4.8435e-04 - acc: 1.0000
Epoch 126/300
166/166 [==============================] - 0s 83us/step - loss: 4.7964e-04 - acc: 1.0000
Epoch 127/300
166/166 [==============================] - 0s 103us/step - loss: 4.6665e-04 - acc: 1.0000
Epoch 128/300
166/166 [==============================] - 0s 84us/step - loss: 4.5798e-04 - acc: 1.0000
Epoch 129/300
166/166 [==============================] - 0s 85us/step - loss: 4.5015e-04 - acc: 1.0000
Epoch 130/300
166/166 [==============================] - 0s 64us/step - loss: 4.4140e-04 - acc: 1.0000
Epoch 131/300
166/166 [==============================] - 0s 71us/step - loss: 4.3318e-04 - acc: 1.0000
Epoch 132/300
166/166 [==============================] - 0s 85us/step - loss: 4.2556e-04 - acc: 1.0000
Epoch 133/300
166/166 [==============================] - 0s 88us/step - loss: 4.1820e-04 - acc: 1.0000
Epoch 134/300
166/166 [==============================] - 0s 88us/step - loss: 4.1008e-04 - acc: 1.0000
Epoch 135/300
166/166 [==============================] - 0s 83us/step - loss: 4.0335e-04 - acc: 1.0000
Epoch 136/300
166/166 [==============================] - 0s 82us/step - loss: 3.9588e-04 - acc: 1.0000
Epoch 137/300
166/166 [==============================] - 0s 71us/step - loss: 3.8880e-04 - acc: 1.0000
Epoch 138/300
166/166 [==============================] - 0s 75us/step - loss: 3.8201e-04 - acc: 1.0000
Epoch 139/300
166/166 [==============================] - 0s 91us/step - loss: 3.7583e-04 - acc: 1.0000
Epoch 140/300
166/166 [==============================] - 0s 88us/step - loss: 3.6868e-04 - acc: 1.0000
Epoch 141/300
166/166 [==============================] - 0s 93us/step - loss: 3.6305e-04 - acc: 1.0000
Epoch 142/300
166/166 [==============================] - 0s 91us/step - loss: 3.5715e-04 - acc: 1.0000
Epoch 143/300
166/166 [==============================] - 0s 97us/step - loss: 3.5082e-04 - acc: 1.0000
Epoch 144/300
166/166 [==============================] - 0s 78us/step - loss: 3.4423e-04 - acc: 1.0000
Epoch 145/300
166/166 [==============================] - 0s 84us/step - loss: 3.3965e-04 - acc: 1.0000
Epoch 146/300
166/166 [==============================] - 0s 79us/step - loss: 3.3292e-04 - acc: 1.0000
Epoch 147/300
166/166 [==============================] - 0s 98us/step - loss: 3.2790e-04 - acc: 1.0000
Epoch 148/300
166/166 [==============================] - 0s 74us/step - loss: 3.2308e-04 - acc: 1.0000
Epoch 149/300
166/166 [==============================] - 0s 86us/step - loss: 3.1709e-04 - acc: 1.0000
Epoch 150/300
166/166 [==============================] - 0s 85us/step - loss: 3.1243e-04 - acc: 1.0000
Epoch 151/300
166/166 [==============================] - 0s 82us/step - loss: 3.0757e-04 - acc: 1.0000
Epoch 152/300
166/166 [==============================] - 0s 83us/step - loss: 3.0246e-04 - acc: 1.0000
Epoch 153/300
166/166 [==============================] - 0s 68us/step - loss: 2.9816e-04 - acc: 1.0000
Epoch 154/300
166/166 [==============================] - 0s 68us/step - loss: 2.9325e-04 - acc: 1.0000
Epoch 155/300
166/166 [==============================] - 0s 84us/step - loss: 2.8906e-04 - acc: 1.0000
Epoch 156/300
166/166 [==============================] - 0s 85us/step - loss: 2.8414e-04 - acc: 1.0000
Epoch 157/300
166/166 [==============================] - 0s 100us/step - loss: 2.8062e-04 - acc: 1.0000
Epoch 158/300
166/166 [==============================] - 0s 90us/step - loss: 2.7661e-04 - acc: 1.0000
Epoch 159/300
166/166 [==============================] - 0s 83us/step - loss: 2.7210e-04 - acc: 1.0000
Epoch 160/300
166/166 [==============================] - 0s 70us/step - loss: 2.6757e-04 - acc: 1.0000
Epoch 161/300
166/166 [==============================] - 0s 64us/step - loss: 2.6383e-04 - acc: 1.0000
Epoch 162/300
166/166 [==============================] - 0s 82us/step - loss: 2.5978e-04 - acc: 1.0000
Epoch 163/300
166/166 [==============================] - 0s 93us/step - loss: 2.5572e-04 - acc: 1.0000
Epoch 164/300
166/166 [==============================] - 0s 86us/step - loss: 2.5230e-04 - acc: 1.0000
Epoch 165/300
166/166 [==============================] - 0s 87us/step - loss: 2.4811e-04 - acc: 1.0000
Epoch 166/300
166/166 [==============================] - 0s 93us/step - loss: 2.4460e-04 - acc: 1.0000
Epoch 167/300
166/166 [==============================] - 0s 70us/step - loss: 2.4098e-04 - acc: 1.0000
Epoch 168/300
166/166 [==============================] - 0s 71us/step - loss: 2.3705e-04 - acc: 1.0000
Epoch 169/300
166/166 [==============================] - 0s 75us/step - loss: 2.3412e-04 - acc: 1.0000
Epoch 170/300
166/166 [==============================] - 0s 78us/step - loss: 2.3029e-04 - acc: 1.0000
Epoch 171/300
166/166 [==============================] - 0s 92us/step - loss: 2.2749e-04 - acc: 1.0000
Epoch 172/300
166/166 [==============================] - 0s 82us/step - loss: 2.2417e-04 - acc: 1.0000
Epoch 173/300
166/166 [==============================] - 0s 80us/step - loss: 2.2075e-04 - acc: 1.0000
Epoch 174/300
166/166 [==============================] - 0s 63us/step - loss: 2.1775e-04 - acc: 1.0000
Epoch 175/300
166/166 [==============================] - 0s 88us/step - loss: 2.1507e-04 - acc: 1.0000
Epoch 176/300
166/166 [==============================] - 0s 79us/step - loss: 2.1182e-04 - acc: 1.0000
Epoch 177/300
166/166 [==============================] - 0s 85us/step - loss: 2.0901e-04 - acc: 1.0000
Epoch 178/300
166/166 [==============================] - 0s 84us/step - loss: 2.0629e-04 - acc: 1.0000
Epoch 179/300
166/166 [==============================] - 0s 68us/step - loss: 2.0376e-04 - acc: 1.0000
Epoch 180/300
166/166 [==============================] - 0s 71us/step - loss: 2.0073e-04 - acc: 1.0000
Epoch 181/300
166/166 [==============================] - 0s 78us/step - loss: 1.9813e-04 - acc: 1.0000
Epoch 182/300
166/166 [==============================] - 0s 85us/step - loss: 1.9568e-04 - acc: 1.0000
Epoch 183/300
166/166 [==============================] - 0s 89us/step - loss: 1.9307e-04 - acc: 1.0000
Epoch 184/300
166/166 [==============================] - 0s 66us/step - loss: 1.9018e-04 - acc: 1.0000
Epoch 185/300
166/166 [==============================] - 0s 58us/step - loss: 1.8780e-04 - acc: 1.0000
Epoch 186/300
166/166 [==============================] - 0s 73us/step - loss: 1.8509e-04 - acc: 1.0000
Epoch 187/300
166/166 [==============================] - 0s 88us/step - loss: 1.8281e-04 - acc: 1.0000
Epoch 188/300
166/166 [==============================] - 0s 105us/step - loss: 1.8010e-04 - acc: 1.0000
Epoch 189/300
166/166 [==============================] - 0s 83us/step - loss: 1.7780e-04 - acc: 1.0000
Epoch 190/300
166/166 [==============================] - 0s 84us/step - loss: 1.7545e-04 - acc: 1.0000
Epoch 191/300
166/166 [==============================] - 0s 89us/step - loss: 1.7333e-04 - acc: 1.0000
Epoch 192/300
166/166 [==============================] - 0s 63us/step - loss: 1.7103e-04 - acc: 1.0000
Epoch 193/300
166/166 [==============================] - 0s 70us/step - loss: 1.6891e-04 - acc: 1.0000
Epoch 194/300
166/166 [==============================] - 0s 81us/step - loss: 1.6683e-04 - acc: 1.0000
Epoch 195/300
166/166 [==============================] - 0s 86us/step - loss: 1.6489e-04 - acc: 1.0000
Epoch 196/300
166/166 [==============================] - 0s 103us/step - loss: 1.6268e-04 - acc: 1.0000
Epoch 197/300
166/166 [==============================] - 0s 92us/step - loss: 1.6047e-04 - acc: 1.0000
Epoch 198/300
166/166 [==============================] - 0s 114us/step - loss: 1.5812e-04 - acc: 1.0000
Epoch 199/300
166/166 [==============================] - 0s 96us/step - loss: 1.5628e-04 - acc: 1.0000
Epoch 200/300
166/166 [==============================] - 0s 103us/step - loss: 1.5487e-04 - acc: 1.0000
Epoch 201/300
166/166 [==============================] - 0s 128us/step - loss: 1.5283e-04 - acc: 1.0000
Epoch 202/300
166/166 [==============================] - 0s 143us/step - loss: 1.5084e-04 - acc: 1.0000
Epoch 203/300
166/166 [==============================] - 0s 98us/step - loss: 1.4895e-04 - acc: 1.0000
Epoch 204/300
166/166 [==============================] - 0s 110us/step - loss: 1.4695e-04 - acc: 1.0000
Epoch 205/300
166/166 [==============================] - 0s 126us/step - loss: 1.4543e-04 - acc: 1.0000
Epoch 206/300
166/166 [==============================] - 0s 111us/step - loss: 1.4370e-04 - acc: 1.0000
Epoch 207/300
166/166 [==============================] - 0s 131us/step - loss: 1.4175e-04 - acc: 1.0000
Epoch 208/300
166/166 [==============================] - 0s 105us/step - loss: 1.4009e-04 - acc: 1.0000
Epoch 209/300
166/166 [==============================] - 0s 109us/step - loss: 1.3851e-04 - acc: 1.0000
Epoch 210/300
166/166 [==============================] - 0s 106us/step - loss: 1.3703e-04 - acc: 1.0000
Epoch 211/300
166/166 [==============================] - 0s 110us/step - loss: 1.3544e-04 - acc: 1.0000
Epoch 212/300
166/166 [==============================] - 0s 146us/step - loss: 1.3392e-04 - acc: 1.0000
Epoch 213/300
166/166 [==============================] - 0s 123us/step - loss: 1.3215e-04 - acc: 1.0000
Epoch 214/300
166/166 [==============================] - 0s 434us/step - loss: 1.3079e-04 - acc: 1.0000
Epoch 215/300
166/166 [==============================] - 0s 95us/step - loss: 1.2920e-04 - acc: 1.0000
Epoch 216/300
166/166 [==============================] - 0s 94us/step - loss: 1.2767e-04 - acc: 1.0000
Epoch 217/300
166/166 [==============================] - 0s 65us/step - loss: 1.2600e-04 - acc: 1.0000
Epoch 218/300
166/166 [==============================] - 0s 89us/step - loss: 1.2464e-04 - acc: 1.0000
Epoch 219/300
166/166 [==============================] - 0s 88us/step - loss: 1.2310e-04 - acc: 1.0000
Epoch 220/300
166/166 [==============================] - 0s 89us/step - loss: 1.2166e-04 - acc: 1.0000
Epoch 221/300
166/166 [==============================] - 0s 95us/step - loss: 1.2036e-04 - acc: 1.0000
Epoch 222/300
166/166 [==============================] - 0s 96us/step - loss: 1.1897e-04 - acc: 1.0000
Epoch 223/300
166/166 [==============================] - 0s 80us/step - loss: 1.1756e-04 - acc: 1.0000
Epoch 224/300
166/166 [==============================] - 0s 86us/step - loss: 1.1634e-04 - acc: 1.0000
Epoch 225/300
166/166 [==============================] - 0s 94us/step - loss: 1.1492e-04 - acc: 1.0000
Epoch 226/300
166/166 [==============================] - 0s 94us/step - loss: 1.1366e-04 - acc: 1.0000
Epoch 227/300
166/166 [==============================] - 0s 109us/step - loss: 1.1241e-04 - acc: 1.0000
Epoch 228/300
166/166 [==============================] - 0s 135us/step - loss: 1.1118e-04 - acc: 1.0000
Epoch 229/300
166/166 [==============================] - 0s 121us/step - loss: 1.0997e-04 - acc: 1.0000
Epoch 230/300
166/166 [==============================] - 0s 104us/step - loss: 1.0880e-04 - acc: 1.0000
Epoch 231/300
166/166 [==============================] - 0s 97us/step - loss: 1.0763e-04 - acc: 1.0000
Epoch 232/300
166/166 [==============================] - 0s 123us/step - loss: 1.0634e-04 - acc: 1.0000
Epoch 233/300
166/166 [==============================] - 0s 95us/step - loss: 1.0517e-04 - acc: 1.0000
Epoch 234/300
166/166 [==============================] - 0s 103us/step - loss: 1.0414e-04 - acc: 1.0000
Epoch 235/300
166/166 [==============================] - 0s 111us/step - loss: 1.0306e-04 - acc: 1.0000
Epoch 236/300
166/166 [==============================] - 0s 119us/step - loss: 1.0199e-04 - acc: 1.0000
Epoch 237/300
166/166 [==============================] - 0s 118us/step - loss: 1.0069e-04 - acc: 1.0000
Epoch 238/300
166/166 [==============================] - 0s 110us/step - loss: 9.9601e-05 - acc: 1.0000
Epoch 239/300
166/166 [==============================] - 0s 101us/step - loss: 9.8541e-05 - acc: 1.0000
Epoch 240/300
166/166 [==============================] - 0s 104us/step - loss: 9.7517e-05 - acc: 1.0000
Epoch 241/300
166/166 [==============================] - 0s 91us/step - loss: 9.6469e-05 - acc: 1.0000
Epoch 242/300
166/166 [==============================] - 0s 82us/step - loss: 9.5525e-05 - acc: 1.0000
Epoch 243/300
166/166 [==============================] - 0s 95us/step - loss: 9.4489e-05 - acc: 1.0000
Epoch 244/300
166/166 [==============================] - 0s 96us/step - loss: 9.3459e-05 - acc: 1.0000
Epoch 245/300
166/166 [==============================] - 0s 81us/step - loss: 9.2447e-05 - acc: 1.0000
Epoch 246/300
166/166 [==============================] - 0s 78us/step - loss: 9.1642e-05 - acc: 1.0000
Epoch 247/300
166/166 [==============================] - 0s 57us/step - loss: 9.0558e-05 - acc: 1.0000
Epoch 248/300
166/166 [==============================] - 0s 76us/step - loss: 8.9651e-05 - acc: 1.0000
Epoch 249/300
166/166 [==============================] - 0s 82us/step - loss: 8.8715e-05 - acc: 1.0000
Epoch 250/300
166/166 [==============================] - 0s 88us/step - loss: 8.7953e-05 - acc: 1.0000
Epoch 251/300
166/166 [==============================] - 0s 91us/step - loss: 8.6915e-05 - acc: 1.0000
Epoch 252/300
166/166 [==============================] - 0s 83us/step - loss: 8.5996e-05 - acc: 1.0000
Epoch 253/300
166/166 [==============================] - 0s 105us/step - loss: 8.5139e-05 - acc: 1.0000
Epoch 254/300
166/166 [==============================] - 0s 65us/step - loss: 8.4277e-05 - acc: 1.0000
Epoch 255/300
166/166 [==============================] - 0s 64us/step - loss: 8.3399e-05 - acc: 1.0000
Epoch 256/300
166/166 [==============================] - 0s 93us/step - loss: 8.2691e-05 - acc: 1.0000
Epoch 257/300
166/166 [==============================] - 0s 82us/step - loss: 8.1701e-05 - acc: 1.0000
Epoch 258/300
166/166 [==============================] - 0s 85us/step - loss: 8.0927e-05 - acc: 1.0000
Epoch 259/300
166/166 [==============================] - 0s 82us/step - loss: 8.0072e-05 - acc: 1.0000
Epoch 260/300
166/166 [==============================] - 0s 60us/step - loss: 7.9307e-05 - acc: 1.0000
Epoch 261/300
166/166 [==============================] - 0s 79us/step - loss: 7.8502e-05 - acc: 1.0000
Epoch 262/300
166/166 [==============================] - 0s 84us/step - loss: 7.7754e-05 - acc: 1.0000
Epoch 263/300
166/166 [==============================] - 0s 83us/step - loss: 7.7005e-05 - acc: 1.0000
Epoch 264/300
166/166 [==============================] - 0s 101us/step - loss: 7.6316e-05 - acc: 1.0000
Epoch 265/300
166/166 [==============================] - 0s 82us/step - loss: 7.5629e-05 - acc: 1.0000
Epoch 266/300
166/166 [==============================] - 0s 79us/step - loss: 7.4787e-05 - acc: 1.0000
Epoch 267/300
166/166 [==============================] - 0s 95us/step - loss: 7.4134e-05 - acc: 1.0000
Epoch 268/300
166/166 [==============================] - 0s 60us/step - loss: 7.3302e-05 - acc: 1.0000
Epoch 269/300
166/166 [==============================] - 0s 90us/step - loss: 7.2539e-05 - acc: 1.0000
Epoch 270/300
166/166 [==============================] - 0s 84us/step - loss: 7.1878e-05 - acc: 1.0000
Epoch 271/300
166/166 [==============================] - 0s 89us/step - loss: 7.1312e-05 - acc: 1.0000
Epoch 272/300
166/166 [==============================] - 0s 87us/step - loss: 7.0449e-05 - acc: 1.0000
Epoch 273/300
166/166 [==============================] - 0s 89us/step - loss: 6.9728e-05 - acc: 1.0000
Epoch 274/300
166/166 [==============================] - 0s 71us/step - loss: 6.9084e-05 - acc: 1.0000
Epoch 275/300
166/166 [==============================] - 0s 70us/step - loss: 6.8411e-05 - acc: 1.0000
Epoch 276/300
166/166 [==============================] - 0s 78us/step - loss: 6.7762e-05 - acc: 1.0000
Epoch 277/300
166/166 [==============================] - 0s 87us/step - loss: 6.7054e-05 - acc: 1.0000
Epoch 278/300
166/166 [==============================] - 0s 88us/step - loss: 6.6382e-05 - acc: 1.0000
Epoch 279/300
166/166 [==============================] - 0s 95us/step - loss: 6.5794e-05 - acc: 1.0000
Epoch 280/300
166/166 [==============================] - 0s 82us/step - loss: 6.5172e-05 - acc: 1.0000
Epoch 281/300
166/166 [==============================] - 0s 74us/step - loss: 6.4489e-05 - acc: 1.0000
Epoch 282/300
166/166 [==============================] - 0s 72us/step - loss: 6.3927e-05 - acc: 1.0000
Epoch 283/300
166/166 [==============================] - 0s 80us/step - loss: 6.3330e-05 - acc: 1.0000
Epoch 284/300
166/166 [==============================] - 0s 92us/step - loss: 6.2621e-05 - acc: 1.0000
Epoch 285/300
166/166 [==============================] - 0s 116us/step - loss: 6.2079e-05 - acc: 1.0000
Epoch 286/300
166/166 [==============================] - 0s 105us/step - loss: 6.1499e-05 - acc: 1.0000
Epoch 287/300
166/166 [==============================] - 0s 95us/step - loss: 6.0944e-05 - acc: 1.0000
Epoch 288/300
166/166 [==============================] - 0s 79us/step - loss: 6.0295e-05 - acc: 1.0000
Epoch 289/300
166/166 [==============================] - 0s 82us/step - loss: 5.9770e-05 - acc: 1.0000
Epoch 290/300
166/166 [==============================] - 0s 83us/step - loss: 5.9147e-05 - acc: 1.0000
Epoch 291/300
166/166 [==============================] - 0s 102us/step - loss: 5.8650e-05 - acc: 1.0000
Epoch 292/300
166/166 [==============================] - 0s 90us/step - loss: 5.8108e-05 - acc: 1.0000
Epoch 293/300
166/166 [==============================] - 0s 77us/step - loss: 5.7514e-05 - acc: 1.0000
Epoch 294/300
166/166 [==============================] - 0s 68us/step - loss: 5.7091e-05 - acc: 1.0000
Epoch 295/300
166/166 [==============================] - 0s 84us/step - loss: 5.6504e-05 - acc: 1.0000
Epoch 296/300
166/166 [==============================] - 0s 84us/step - loss: 5.6007e-05 - acc: 1.0000
Epoch 297/300
166/166 [==============================] - 0s 86us/step - loss: 5.5437e-05 - acc: 1.0000
Epoch 298/300
166/166 [==============================] - 0s 84us/step - loss: 5.4989e-05 - acc: 1.0000
Epoch 299/300
166/166 [==============================] - 0s 77us/step - loss: 5.4519e-05 - acc: 1.0000
Epoch 300/300
166/166 [==============================] - 0s 65us/step - loss: 5.3995e-05 - acc: 1.0000

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} <keras.callbacks.History at 0x7f15545aacf8>
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{test\PYZus{}loss} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{xTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}loss}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0.7980401374044872, 0.8333333276567005]

    \end{Verbatim}

    \hypertarget{set-early-stopping-monitor-so-the-model-stops-training-when-it-wont-improve-anymore}{%
\subsection{Set early stopping monitor so the model stops training when
it won't improve
anymore}\label{set-early-stopping-monitor-so-the-model-stops-training-when-it-wont-improve-anymore}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{EarlyStopping}\PY{p}{,} \PY{n}{ModelCheckpoint}
         \PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{keras\PYZus{}callbacks}\PY{o}{=}\PY{p}{[}
             \PY{n}{EarlyStopping}\PY{p}{(}\PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{,}
             \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{p}{,}\PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{p}{]}
\end{Verbatim}


    \hypertarget{train-model}{%
\subsection{Train model}\label{train-model}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{Baseline\PYZus{}Model}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{n}{keras\PYZus{}callbacks}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 132 samples, validate on 34 samples
Epoch 1/300
132/132 [==============================] - 0s 2ms/step - loss: 0.6544 - acc: 0.5985 - val\_loss: 0.6078 - val\_acc: 0.7647
Epoch 2/300
132/132 [==============================] - 0s 98us/step - loss: 0.5554 - acc: 0.7500 - val\_loss: 0.5267 - val\_acc: 0.7647
Epoch 3/300
132/132 [==============================] - 0s 110us/step - loss: 0.4868 - acc: 0.8106 - val\_loss: 0.4793 - val\_acc: 0.8235
Epoch 4/300
132/132 [==============================] - 0s 91us/step - loss: 0.4345 - acc: 0.8788 - val\_loss: 0.4344 - val\_acc: 0.8529
Epoch 5/300
132/132 [==============================] - 0s 112us/step - loss: 0.3879 - acc: 0.8788 - val\_loss: 0.3906 - val\_acc: 0.8529
Epoch 6/300
132/132 [==============================] - 0s 98us/step - loss: 0.3425 - acc: 0.8939 - val\_loss: 0.3553 - val\_acc: 0.8529
Epoch 7/300
132/132 [==============================] - 0s 101us/step - loss: 0.3021 - acc: 0.9318 - val\_loss: 0.3341 - val\_acc: 0.8529
Epoch 8/300
132/132 [==============================] - 0s 105us/step - loss: 0.2664 - acc: 0.9394 - val\_loss: 0.3198 - val\_acc: 0.8529
Epoch 9/300
132/132 [==============================] - 0s 69us/step - loss: 0.2344 - acc: 0.9394 - val\_loss: 0.3097 - val\_acc: 0.8824
Epoch 10/300
132/132 [==============================] - 0s 57us/step - loss: 0.2036 - acc: 0.9848 - val\_loss: 0.2985 - val\_acc: 0.8824
Epoch 11/300
132/132 [==============================] - 0s 69us/step - loss: 0.1758 - acc: 0.9924 - val\_loss: 0.2929 - val\_acc: 0.8824
Epoch 12/300
132/132 [==============================] - 0s 68us/step - loss: 0.1525 - acc: 1.0000 - val\_loss: 0.2852 - val\_acc: 0.8824
Epoch 13/300
132/132 [==============================] - 0s 81us/step - loss: 0.1324 - acc: 1.0000 - val\_loss: 0.2898 - val\_acc: 0.8824
Epoch 14/300
132/132 [==============================] - 0s 64us/step - loss: 0.1147 - acc: 1.0000 - val\_loss: 0.3065 - val\_acc: 0.8824
Epoch 15/300
132/132 [==============================] - 0s 141us/step - loss: 0.1001 - acc: 1.0000 - val\_loss: 0.3092 - val\_acc: 0.8824
Epoch 16/300
132/132 [==============================] - 0s 65us/step - loss: 0.0869 - acc: 1.0000 - val\_loss: 0.3068 - val\_acc: 0.8824
Epoch 17/300
132/132 [==============================] - 0s 78us/step - loss: 0.0748 - acc: 1.0000 - val\_loss: 0.3105 - val\_acc: 0.8824
Epoch 18/300
132/132 [==============================] - 0s 82us/step - loss: 0.0662 - acc: 1.0000 - val\_loss: 0.3126 - val\_acc: 0.8824
Epoch 19/300
132/132 [==============================] - 0s 118us/step - loss: 0.0595 - acc: 1.0000 - val\_loss: 0.3243 - val\_acc: 0.8824
Epoch 20/300
132/132 [==============================] - 0s 72us/step - loss: 0.0517 - acc: 1.0000 - val\_loss: 0.3262 - val\_acc: 0.8824
Epoch 21/300
132/132 [==============================] - 0s 84us/step - loss: 0.0452 - acc: 1.0000 - val\_loss: 0.3294 - val\_acc: 0.8824
Epoch 22/300
132/132 [==============================] - 0s 125us/step - loss: 0.0412 - acc: 1.0000 - val\_loss: 0.3192 - val\_acc: 0.8824
Epoch 23/300
132/132 [==============================] - 0s 117us/step - loss: 0.0364 - acc: 1.0000 - val\_loss: 0.3310 - val\_acc: 0.8824
Epoch 24/300
132/132 [==============================] - 0s 107us/step - loss: 0.0319 - acc: 1.0000 - val\_loss: 0.3518 - val\_acc: 0.8824
Epoch 25/300
132/132 [==============================] - 0s 154us/step - loss: 0.0290 - acc: 1.0000 - val\_loss: 0.3561 - val\_acc: 0.8824
Epoch 26/300
132/132 [==============================] - 0s 130us/step - loss: 0.0259 - acc: 1.0000 - val\_loss: 0.3575 - val\_acc: 0.8824
Epoch 27/300
132/132 [==============================] - 0s 137us/step - loss: 0.0232 - acc: 1.0000 - val\_loss: 0.3583 - val\_acc: 0.8824
Epoch 28/300
132/132 [==============================] - 0s 106us/step - loss: 0.0210 - acc: 1.0000 - val\_loss: 0.3628 - val\_acc: 0.8824
Epoch 29/300
132/132 [==============================] - 0s 113us/step - loss: 0.0192 - acc: 1.0000 - val\_loss: 0.3677 - val\_acc: 0.8824
Epoch 30/300
132/132 [==============================] - 0s 90us/step - loss: 0.0176 - acc: 1.0000 - val\_loss: 0.3724 - val\_acc: 0.8824
Epoch 31/300
132/132 [==============================] - 0s 108us/step - loss: 0.0160 - acc: 1.0000 - val\_loss: 0.3796 - val\_acc: 0.8824
Epoch 32/300
132/132 [==============================] - 0s 105us/step - loss: 0.0146 - acc: 1.0000 - val\_loss: 0.3803 - val\_acc: 0.8824
Epoch 33/300
132/132 [==============================] - 0s 113us/step - loss: 0.0136 - acc: 1.0000 - val\_loss: 0.3832 - val\_acc: 0.8824
Epoch 34/300
132/132 [==============================] - 0s 83us/step - loss: 0.0124 - acc: 1.0000 - val\_loss: 0.3909 - val\_acc: 0.8824
Epoch 35/300
132/132 [==============================] - 0s 116us/step - loss: 0.0115 - acc: 1.0000 - val\_loss: 0.3948 - val\_acc: 0.8824
Epoch 36/300
132/132 [==============================] - 0s 104us/step - loss: 0.0105 - acc: 1.0000 - val\_loss: 0.4004 - val\_acc: 0.8824
Epoch 37/300
132/132 [==============================] - 0s 164us/step - loss: 0.0098 - acc: 1.0000 - val\_loss: 0.4037 - val\_acc: 0.8824
Epoch 38/300
132/132 [==============================] - 0s 90us/step - loss: 0.0090 - acc: 1.0000 - val\_loss: 0.4198 - val\_acc: 0.8824
Epoch 39/300
132/132 [==============================] - 0s 96us/step - loss: 0.0085 - acc: 1.0000 - val\_loss: 0.4264 - val\_acc: 0.8824
Epoch 40/300
132/132 [==============================] - 0s 117us/step - loss: 0.0079 - acc: 1.0000 - val\_loss: 0.4294 - val\_acc: 0.8824
Epoch 41/300
132/132 [==============================] - 0s 119us/step - loss: 0.0073 - acc: 1.0000 - val\_loss: 0.4353 - val\_acc: 0.8824
Epoch 42/300
132/132 [==============================] - 0s 113us/step - loss: 0.0068 - acc: 1.0000 - val\_loss: 0.4367 - val\_acc: 0.8824

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} <keras.callbacks.History at 0x7f45982a03c8>
\end{Verbatim}
            
    Here we can see that training accuracy=100 \% but validation accuracy is
88\% so the model is overfit. To overcome this problem we use the
dropout

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{constraints} \PY{k}{import} \PY{n}{maxnorm}
         
         \PY{k}{def} \PY{n+nf}{create\PYZus{}model\PYZus{}dropOut}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} create model}
             \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{60}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{60}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Compile model}
             \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{k}{return} \PY{n}{model}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{create\PYZus{}model\PYZus{}dropOut}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{n}{keras\PYZus{}callbacks}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 132 samples, validate on 34 samples
Epoch 1/300
132/132 [==============================] - 1s 5ms/step - loss: 0.7610 - acc: 0.5303 - val\_loss: 0.5990 - val\_acc: 0.6765
Epoch 2/300
132/132 [==============================] - 0s 96us/step - loss: 0.6888 - acc: 0.5530 - val\_loss: 0.5626 - val\_acc: 0.7059
Epoch 3/300
132/132 [==============================] - 0s 149us/step - loss: 0.6222 - acc: 0.6439 - val\_loss: 0.5399 - val\_acc: 0.7647
Epoch 4/300
132/132 [==============================] - 0s 132us/step - loss: 0.5502 - acc: 0.6894 - val\_loss: 0.5169 - val\_acc: 0.7941
Epoch 5/300
132/132 [==============================] - 0s 126us/step - loss: 0.5554 - acc: 0.7197 - val\_loss: 0.4929 - val\_acc: 0.8235
Epoch 6/300
132/132 [==============================] - 0s 124us/step - loss: 0.5633 - acc: 0.7121 - val\_loss: 0.4699 - val\_acc: 0.8235
Epoch 7/300
132/132 [==============================] - 0s 138us/step - loss: 0.5274 - acc: 0.7121 - val\_loss: 0.4507 - val\_acc: 0.8235
Epoch 8/300
132/132 [==============================] - 0s 104us/step - loss: 0.5070 - acc: 0.7424 - val\_loss: 0.4371 - val\_acc: 0.8235
Epoch 9/300
132/132 [==============================] - 0s 138us/step - loss: 0.4704 - acc: 0.8182 - val\_loss: 0.4236 - val\_acc: 0.8529
Epoch 10/300
132/132 [==============================] - 0s 108us/step - loss: 0.4330 - acc: 0.8030 - val\_loss: 0.4119 - val\_acc: 0.8529
Epoch 11/300
132/132 [==============================] - 0s 136us/step - loss: 0.4615 - acc: 0.8030 - val\_loss: 0.4029 - val\_acc: 0.8529
Epoch 12/300
132/132 [==============================] - 0s 122us/step - loss: 0.4212 - acc: 0.8182 - val\_loss: 0.3962 - val\_acc: 0.8529
Epoch 13/300
132/132 [==============================] - 0s 145us/step - loss: 0.4010 - acc: 0.8258 - val\_loss: 0.3897 - val\_acc: 0.8824
Epoch 14/300
132/132 [==============================] - 0s 138us/step - loss: 0.4184 - acc: 0.8409 - val\_loss: 0.3825 - val\_acc: 0.8824
Epoch 15/300
132/132 [==============================] - 0s 117us/step - loss: 0.3616 - acc: 0.8788 - val\_loss: 0.3741 - val\_acc: 0.8824
Epoch 16/300
132/132 [==============================] - 0s 118us/step - loss: 0.3781 - acc: 0.8409 - val\_loss: 0.3649 - val\_acc: 0.8824
Epoch 17/300
132/132 [==============================] - 0s 151us/step - loss: 0.3568 - acc: 0.8788 - val\_loss: 0.3579 - val\_acc: 0.8824
Epoch 18/300
132/132 [==============================] - 0s 137us/step - loss: 0.3147 - acc: 0.9167 - val\_loss: 0.3522 - val\_acc: 0.8824
Epoch 19/300
132/132 [==============================] - 0s 147us/step - loss: 0.3151 - acc: 0.9167 - val\_loss: 0.3475 - val\_acc: 0.8824
Epoch 20/300
132/132 [==============================] - 0s 112us/step - loss: 0.3376 - acc: 0.8939 - val\_loss: 0.3433 - val\_acc: 0.8824
Epoch 21/300
132/132 [==============================] - 0s 109us/step - loss: 0.2968 - acc: 0.8788 - val\_loss: 0.3423 - val\_acc: 0.8824
Epoch 22/300
132/132 [==============================] - 0s 205us/step - loss: 0.3004 - acc: 0.8636 - val\_loss: 0.3419 - val\_acc: 0.8824
Epoch 23/300
132/132 [==============================] - 0s 134us/step - loss: 0.2949 - acc: 0.8712 - val\_loss: 0.3409 - val\_acc: 0.9118
Epoch 24/300
132/132 [==============================] - 0s 149us/step - loss: 0.2627 - acc: 0.9242 - val\_loss: 0.3409 - val\_acc: 0.9118
Epoch 25/300
132/132 [==============================] - 0s 134us/step - loss: 0.2825 - acc: 0.8712 - val\_loss: 0.3428 - val\_acc: 0.9118
Epoch 26/300
132/132 [==============================] - 0s 125us/step - loss: 0.2222 - acc: 0.9697 - val\_loss: 0.3421 - val\_acc: 0.9118
Epoch 27/300
132/132 [==============================] - 0s 133us/step - loss: 0.2307 - acc: 0.9091 - val\_loss: 0.3368 - val\_acc: 0.9118
Epoch 28/300
132/132 [==============================] - 0s 143us/step - loss: 0.2325 - acc: 0.9394 - val\_loss: 0.3345 - val\_acc: 0.9118
Epoch 29/300
132/132 [==============================] - 0s 135us/step - loss: 0.2454 - acc: 0.9167 - val\_loss: 0.3325 - val\_acc: 0.9118
Epoch 30/300
132/132 [==============================] - 0s 159us/step - loss: 0.2230 - acc: 0.9167 - val\_loss: 0.3336 - val\_acc: 0.9118
Epoch 31/300
132/132 [==============================] - 0s 109us/step - loss: 0.2025 - acc: 0.9773 - val\_loss: 0.3352 - val\_acc: 0.9118
Epoch 32/300
132/132 [==============================] - 0s 105us/step - loss: 0.2046 - acc: 0.9318 - val\_loss: 0.3411 - val\_acc: 0.9118
Epoch 33/300
132/132 [==============================] - 0s 139us/step - loss: 0.2222 - acc: 0.9242 - val\_loss: 0.3480 - val\_acc: 0.9118
Epoch 34/300
132/132 [==============================] - 0s 139us/step - loss: 0.1882 - acc: 0.9470 - val\_loss: 0.3522 - val\_acc: 0.9118
Epoch 35/300
132/132 [==============================] - 0s 135us/step - loss: 0.2084 - acc: 0.9318 - val\_loss: 0.3536 - val\_acc: 0.9118
Epoch 36/300
132/132 [==============================] - 0s 139us/step - loss: 0.1920 - acc: 0.9318 - val\_loss: 0.3531 - val\_acc: 0.9118
Epoch 37/300
132/132 [==============================] - 0s 140us/step - loss: 0.1685 - acc: 0.9545 - val\_loss: 0.3539 - val\_acc: 0.9118
Epoch 38/300
132/132 [==============================] - 0s 160us/step - loss: 0.1358 - acc: 0.9773 - val\_loss: 0.3584 - val\_acc: 0.9118
Epoch 39/300
132/132 [==============================] - 0s 159us/step - loss: 0.1631 - acc: 0.9621 - val\_loss: 0.3597 - val\_acc: 0.9118
Epoch 40/300
132/132 [==============================] - 0s 138us/step - loss: 0.1581 - acc: 0.9621 - val\_loss: 0.3568 - val\_acc: 0.9118
Epoch 41/300
132/132 [==============================] - 0s 185us/step - loss: 0.1665 - acc: 0.9470 - val\_loss: 0.3526 - val\_acc: 0.9118
Epoch 42/300
132/132 [==============================] - 0s 187us/step - loss: 0.1323 - acc: 0.9848 - val\_loss: 0.3487 - val\_acc: 0.9118
Epoch 43/300
132/132 [==============================] - 0s 181us/step - loss: 0.1404 - acc: 0.9621 - val\_loss: 0.3442 - val\_acc: 0.9118
Epoch 44/300
132/132 [==============================] - 0s 193us/step - loss: 0.1087 - acc: 0.9697 - val\_loss: 0.3413 - val\_acc: 0.9118
Epoch 45/300
132/132 [==============================] - 0s 148us/step - loss: 0.1119 - acc: 0.9773 - val\_loss: 0.3429 - val\_acc: 0.9118
Epoch 46/300
132/132 [==============================] - 0s 165us/step - loss: 0.1181 - acc: 0.9697 - val\_loss: 0.3451 - val\_acc: 0.9118
Epoch 47/300
132/132 [==============================] - 0s 220us/step - loss: 0.1198 - acc: 0.9697 - val\_loss: 0.3438 - val\_acc: 0.9118
Epoch 48/300
132/132 [==============================] - 0s 195us/step - loss: 0.0966 - acc: 0.9773 - val\_loss: 0.3449 - val\_acc: 0.9118
Epoch 49/300
132/132 [==============================] - 0s 158us/step - loss: 0.1108 - acc: 0.9621 - val\_loss: 0.3492 - val\_acc: 0.9118
Epoch 50/300
132/132 [==============================] - 0s 151us/step - loss: 0.0809 - acc: 0.9848 - val\_loss: 0.3569 - val\_acc: 0.9118
Epoch 51/300
132/132 [==============================] - 0s 161us/step - loss: 0.0942 - acc: 0.9848 - val\_loss: 0.3619 - val\_acc: 0.9118
Epoch 52/300
132/132 [==============================] - 0s 152us/step - loss: 0.1107 - acc: 0.9470 - val\_loss: 0.3663 - val\_acc: 0.9118
Epoch 53/300
132/132 [==============================] - 0s 156us/step - loss: 0.0900 - acc: 0.9773 - val\_loss: 0.3701 - val\_acc: 0.9118
Epoch 54/300
132/132 [==============================] - 0s 154us/step - loss: 0.1102 - acc: 0.9621 - val\_loss: 0.3717 - val\_acc: 0.9118
Epoch 55/300
132/132 [==============================] - 0s 155us/step - loss: 0.0807 - acc: 0.9924 - val\_loss: 0.3749 - val\_acc: 0.9118
Epoch 56/300
132/132 [==============================] - 0s 207us/step - loss: 0.0792 - acc: 0.9848 - val\_loss: 0.3760 - val\_acc: 0.9118
Epoch 57/300
132/132 [==============================] - 0s 96us/step - loss: 0.0836 - acc: 0.9848 - val\_loss: 0.3737 - val\_acc: 0.9118
Epoch 58/300
132/132 [==============================] - 0s 123us/step - loss: 0.0718 - acc: 0.9848 - val\_loss: 0.3679 - val\_acc: 0.9118
Epoch 59/300
132/132 [==============================] - 0s 127us/step - loss: 0.0915 - acc: 0.9697 - val\_loss: 0.3648 - val\_acc: 0.9118

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:} <keras.callbacks.History at 0x7f4580650160>
\end{Verbatim}
            
    Now Load

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{load\PYZus{}model}
         \PY{n}{saved\PYZus{}model} \PY{o}{=} \PY{n}{load\PYZus{}model}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}acc} \PY{o}{=} \PY{n}{saved\PYZus{}model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{test\PYZus{}acc} \PY{o}{=} \PY{n}{saved\PYZus{}model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{xTest}\PY{p}{,} \PY{n}{yTest}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{train\PYZus{}acc}\PY{p}{,} \PY{n}{test\PYZus{}acc}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0.1280785138853433, 0.9759036144578314] [0.424984933364959, 0.8095238123621259]

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
